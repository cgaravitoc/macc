{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase 8: Introducción a las redes Neuronales\n",
    "\n",
    "En esta clase vamos a dar una introducción al concepto de Redes Neuronales. Empezaremos desde una perspectiva histórica , y primero abordaremos lo que es la Neurona de McCulloch-Pitts el perceptron y el adaline. \n",
    "\n",
    "## Motivación\n",
    "\n",
    "**Pregunta:** ¿Cuál es la ventaja de Redes Neuronales sobre los métodos de regresión y clasificación lineal que hemos visto?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** Los métodos de regresión lineal suponen que conocemos como debe ser la superficie de separación, o el orden de la regresión. Sin embargo, si tenemos este conocimiento, ¿Cómo podemos saber qué orden de polinomio debemos utilizar, o que transformación no-lineal se debe aplicar a los regresores, para ajustar los datos a nuestro modelo? Es más, si suponemos que tenemos 100 regresores de entrada, para poder ajustar los datos a un polinomio cuadrático, necesitaremos aproximadamente 5000 combinaciones no lineales de estos regresores, y para uno de orden tres, alrededor de 500000, lo cual se vuelve muy dispendioso y generaría matrices muy grandes. Por esta razón es necesario utilizar un método que permita realizar una transformación no-lineal sin tener un crecimiento exagerado en el número de regresores utilizados.  Además, cuando se tienen muchas variables de entrada, es muy difícil poder encontrar la relación que hay entre los regresores y la salida (ya sea clasificación o regresión).\n",
    "\n",
    "\n",
    "<img src=\"img/NL_Clas.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neuronales Intro\n",
    "\n",
    "Las redes neuronales surgieron de algoritmos que tratan de imitar el cerebro. Son una simplificación de la forma como las redes neuronales funcionan. A pesar de que el comportamiento del cerbero pareciera que fuera muy complicado existe una hipótesis que indica que existe una sola forma como el cerebro aprende. Esta hipótesis está basada en la plasticidad del cerebro *(The one learning algorihtm Hypothesis)*. La plasticidad del cerebro es la capacidad que tiene el cerebro para aprender a realizar ciertas tareas con otras áreas que no están dedicadas a esas tareas. Por ejemplo, cuando una parete del cerebro es removida debido a un tumor cerebral o una enfermedad, el paciente puede recuperar la habilidad perdida por medio de terapia. Experimentos en esta área se conocen como Neuro-rewiring.\n",
    "\n",
    "Primero veamos un video sobre neuro-plasticity [aquí](https://www.youtube.com/watch?v=ELpfYCZa87g).\n",
    "\n",
    "Ahora uno sobre neuro-rewiring [aquí](https://www.youtube.com/watch?v=4c1lqFXHvqI&ab_channel=TED). *\"Si uno conecta un sensor al cerebro, el cerebro aprenderá a manejar la información que se le suministra\"*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos de Neuronas\n",
    "\n",
    "La neurona es la célula básica del sistema nervioso, el cerebro esta compuesto por un conjunto de neuronas que estan conectadas entre si. En la figura de abajo se presenta un modelo de una neurona.\n",
    "\n",
    "<img src=\"img/Neurona.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La Neurona de McCulloch-Pitts\n",
    "\n",
    "La neurona de McCulloch-Pitts fue desarrollada por Warren MuCulloch y Walter Pitts, un neurofisiólogo y un lógico, en 1943 (ambos también eran ingenieros electrónicos). Esta neurona es una representación matemática **MUY ABSTRACTA** de lo que es el funcionamiento de una neurona biologíca. Aún así este modelo fue inspirado en las neuronas biológicas.\n",
    "\n",
    "El modelo de neurona de McCulloch-Pitts (MCP) supone que la salida de la neurona es una suma de sus entradas multiplicadas por unos pesos, los cuales son todos iguales:\n",
    "\n",
    "$$g(x_1,x_2,\\ldots,x_n) = g(\\mathbf{x}) = \\sum_{i = 1}^n \\omega x_i,$$ \n",
    "\n",
    "donde $\\omega$ es un número real positivo. La salida de esta neurona atravieza una función de una función de umbral del tipo:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  y = f(g(\\mathbf{x}))=\n",
    "    \\begin{cases}\n",
    "      1 & \\text{if} & g(\\mathbf{x}) \\geq \\mu\\\\\n",
    "      0 & \\text{if} & g(\\mathbf{x}) < \\mu\n",
    "    \\end{cases}       \n",
    "\\end{equation}$$\n",
    "\n",
    "Donde $\\mu$ es un umbral. La representación matemática de esta neurona esta dada por la siguiente figura:\n",
    "\n",
    "<img src=\"img/MCP.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las suposiciones del modelo MCP son:\n",
    "\n",
    "1. Las entradas son binarias.\n",
    "2. Cada neurona tiene un valor de umbral fijo.\n",
    "3. Todas las entradas tienen pesos identicos $\\omega$.\n",
    "4. Entradas inhibitorias tienen un veto absoluto sobre la salida.\n",
    "5. A cada momento la salida de la neurona se actualiza y arroja un uno si la suma de las entradas supera el umbral.\n",
    "\n",
    "Debido a que este modelo no era muy flexible y no tiene un algoritmo de aprendizaje no fue muy popular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El Perceptrón\n",
    "\n",
    "el perceptron es una mejora de la neurona MPC, las diferencias son:\n",
    "\n",
    "1. Los pesos y los umbrales pueden ser diferentes.\n",
    "2. Los pesos pueden ser positivos o negativos.\n",
    "3. No hay entrada inhibitoria.\n",
    "4. La salida va de $-1$ a $1$, no de $0$ a $1$. \n",
    "5. Existe regla de aprendizaje.\n",
    "\n",
    "El perceptrón fue desarrollado por Frank Rosenblatt en 1957.  El modelo del perceptrón se puede ver en la siguiente figura:\n",
    "\n",
    "<img src=\"img/perceptron.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida de un perceptron esta dada por $z = \\boldsymbol\\omega^{\\text{T}}\\mathbf{x}$, esta salida se alimenta a una función de umbral para encontrar la salida:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  y = \\varphi(z)=\n",
    "    \\begin{cases}\n",
    "      1 & \\text{if} & z \\geq \\theta\\\\\n",
    "      -1 & \\text{if} & z < \\theta\n",
    "    \\end{cases}       \n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regla de aprendizaje del perceptrón\n",
    "\n",
    "El aprendizaje del perceptrón se divide en dos pasos:\n",
    "\n",
    "1. Inicializar los pesos $\\omega$ a 0 o a números aleatorios pequeños.\n",
    "2. Para cada dato de entrenamiento $\\mathbf{x}^{(i)}$ realizar los siguientes pasos:\n",
    "    * actualizar le valor de la predicción, $\\hat{y}$\n",
    "    * Actualizar los pesos\n",
    "    \n",
    "De forma escalar esa actualización esta dada por\n",
    "\n",
    "$$\\omega_j := \\omega_j+\\Delta\\omega_j,$$\n",
    "\n",
    "para $j=1,\\ldots,m$, y \n",
    "\n",
    "$$\\Delta\\omega_j = \\eta(y^{(i)}-\\hat{y}^{(i)}) x_j^{(i)},$$\n",
    "donde $\\eta$ es la constante de aprendizaje.\n",
    "\n",
    "En forma vectorial las ecuaciones de aprendizaje son:\n",
    "\n",
    "$$\\boldsymbol\\omega := \\boldsymbol\\omega+\\Delta\\boldsymbol\\omega,$$\n",
    "\n",
    "$$\\Delta\\boldsymbol\\omega = \\eta\\mathbf{X}^{\\text{T}}(\\mathbf{y}-\\hat{\\mathbf{y}}),$$\n",
    "\n",
    "donde $\\mathbf{X}$ es una matrix de dimensiones $m\\times N$, donde $m$ es el número de observaciones, y $N$ el número de entradas (regresores, cvaracteristicas, etc). Normalmente también se agrega un vector de unos como en los casos anteriores, esto indica que tenemos un vector de unos en la matrix $\\mathbf{X}$.\n",
    "\n",
    "**Pregunta:** ¿Cómo interpreta usted esa regla de aprendizaje?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** Podemos observar que si la predicción es correcta no se hace ninguna actualiación en los pesos. Si la clase a predecir era $1$, y se predijo la clase $-1$, al vector de pesos se le suma una version escalada de esa entrada que se predijo mal, de esta forma el vector de pesos se va a *parecer* más a los datos de entrada que predijo mal. Por otro lado si la clase a predecir era $-1$ y se predice $1$, entonces el vector de pesos se aleja de ese dato y se dirije a la dirección contraria de este, esto para parecerse lo menos posible a ese dato. Los parámetros (pesos) almacenan información sobre los datos que clasifica en función de como es su producto punto, y tienden a parecerse a los datos para la clase $1$ y alejarse lo más posible a los datos de la clase $-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regla de aprendizaje del perceptron se puede resumir en el siguiente esquema:\n",
    "\n",
    "<img src=\"img/perceptron_aprendizaje.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitaciones del perceptrón\n",
    "\n",
    "El pereceptrón tiene las siguientes limitaciones:\n",
    "\n",
    "1. No generaliza bien. Puedo ingresar tanto valores de entada para aprender una función booleana.\n",
    "2. El perceptron esta limitado a separar clases de funciones que son linealmente separables.\n",
    "3. La función de aprendizaje es empoirica, no tiene una base matemática solida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADALINE ( ADAptive LInear NEuron)\n",
    "\n",
    "El adaline es un modelo de red neuronal artifical desarrollado por el profesor Bernard Widrow y Ted Hoof en Stanford en 1960. La arquitectura del adaline se presenta a continuación:\n",
    "\n",
    "<img src=\"img/adaline_aprendizaje.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, en este caso el error se calcula antes de entrar a la función de umbral, esto permite que los errores de predicción sean continuos y no discretos.\n",
    "\n",
    "**Pregunta:** ¿Qué ventaja tiene esto sobre el perceptrón?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** Al tener errores continuos y no discretos, es posible calcular sus gradientes y utilizar gradiente descendiente para actualizar los pesos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La formula por gradiente descendiente para actualizar los pesos es:\n",
    "\n",
    "$$\\boldsymbol\\omega:=\\boldsymbol\\omega-\\eta\\nabla\\mathbf{J}(\\boldsymbol\\omega).$$\n",
    "\n",
    "Donde se puede definir $\\Delta\\boldsymbol\\omega = -\\eta\\nabla\\mathbf{J}(\\boldsymbol\\omega)$. Derivando en función de $\\omega_j$ la función de costo obtenemos:\n",
    "\n",
    "$$\\frac{\\partial\\mathbf{J}}{\\partial\\omega_j}= -\\sum_{i = 1}^{m}\\left(y^{(i)}-\\varphi(z^{(i)})\\right)x_j^{(i)}.$$\n",
    "\n",
    "Como $\\varphi(z) = \\varphi(\\boldsymbol\\omega^{\\text{T}}\\mathbf{x}) = \\boldsymbol\\omega^{\\text{T}}\\mathbf{x}$, entonces:\n",
    "\n",
    "$$\\Delta\\omega_j = \\eta\\sum_{i=1}^m (y^{(i)}-\\boldsymbol\\omega^{\\text{T}}\\mathbf{x})x_j^{(i)}.$$\n",
    "\n",
    "En forma vectorial:\n",
    "\n",
    "$$\\Delta\\boldsymbol\\omega = \\eta\\mathbf{X}^{\\text{T}}(\\mathbf{y}-\\mathbf{X}\\boldsymbol\\omega)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma, para adaline, la función de costo esta dada por el error cuadratico del \"*error de predicción*\":\n",
    "$$ \\mathbf{J}(\\boldsymbol\\omega)= \\frac{1}{2}\\sum_{i=1}^m(y^{(i)}-\\varphi(z^{(i)}))^2,$$\n",
    "\n",
    "donde $z = \\boldsymbol\\omega^{\\text{T}}\\mathbf{x}$, y $\\varphi(z) = z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** ¿Cómo interpreta de forma geométrica esta regla de actualización?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** Adaline trata de realizar una regresión lineal entre los datos de entrada y la salida, y en función del error qu ese genera así mismo actualiza los pesos, de tal forma que mueve los pesos en una dirección que seria l amedia ponderada de los errores. Cuando el algoritmo de aprendizaje converge, el error medio entre la predicción y el modelo de regresión lineal debe ser cero. De esta forma los pesos no se actualizan más."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAREA\n",
    "\n",
    "\n",
    "\n",
    "1. Implementación de un perceptron y su regla de aprendizaje. El perceptron debe aceptar como entrada un vector de longittud arbitraria, i.e. el número de caracteristicas (regresores) puede ser arbitrario.\n",
    "\n",
    "2. Implementación de un adaline con su regla de aprendizaje. El adaline debe aceptar como entrada un vector de longitud arbitraria, i.e. el número de caracteristicas (regresores) puede ser arbitrario.\n",
    "\n",
    "3. Implemente un perceptrón y un adaline que pueda realizar las operaciones logicas AND, OR, XOR y XNOR.\n",
    "\n",
    "4. Al implementar la AND y la OR, si compara los pesos con lso datos de la clase $1$ (cuya salida es $1$, ¿Qué puede concluir?\n",
    "\n",
    "5. Solucione el problema de las flores de iris utilizando un perceptrón y un adaline. Utilice solamente dos clases. Compare las respuestas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
