{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./imagenes/Macc.png\" width=\"400\"/></td>\n",
    "        <td>&nbsp;</td>\n",
    "        <td>\n",
    "            <h1 style=\"color:blue;text-align:left\">Inteligencia Artificial</h1></td>\n",
    "        <td>\n",
    "            <table><tr>\n",
    "            <tp><p style=\"font-size:150%;text-align:center\">Notebook 7</p></tp>\n",
    "            <tp><p style=\"font-size:150%;text-align:center\">Arquitecturas de agentes</p></tp>\n",
    "            </tr></table>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo <a class=\"anchor\" id=\"inicio\"></a>\n",
    "\n",
    "En este notebook ejemplificaremos tres tipos de agente: dirigido por tabla, de respuesta simple y basado en el conocimiento. Usaremos el problema del laberinto para implementar los tres tipos de programa de agente para intentar salir del laberinto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from agentes import Laberinto\n",
    "\n",
    "lab = Laberinto(pos_inicial=(11,11))\n",
    "lab.pintar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secciones\n",
    "\n",
    "Desarrollaremos la explicación en las siguientes secciones:\n",
    "\n",
    "1. [El ambiente del laberinto](#lab)\n",
    "2. [Un agente dirigido por tabla](#agenteTD)\n",
    "3. [Un agente de reflejo simple](#agenteSR)\n",
    "4. [Un agente basado en el conocimiento](#agenteKB)\n",
    "5. [Creando un mapa mental](#mapa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El ambiente del laberinto <a class=\"anchor\" id=\"lab\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "En el problema del laberinto la percepción del agente está basada en una colección de sensores que se encienden cuando detectan un obstáculo, organizados de la siguiente manera:\n",
    "\n",
    "`[sensor frontal, sensor izquierdo, sensor derecho, sensor trasero]`\n",
    "\n",
    "Los valores de los sensores se obtienen mediante el método `para_sentidos()` de la clase `Laberinto` y se almacenan en el atributo `perceptos` de un objeto de clase `Agente`. Veamos la implementación de la percepción del agente directamente en el ejemplo donde el agente comienza en la casilla $(11,11)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La lista de sensores es:\n",
      "[sensor frontal, sensor izquierdo, sensor derecho, sensor trasero]\n",
      "Los valores en la casilla (11,11) son:\n",
      "[False, False, True, True]\n"
     ]
    }
   ],
   "source": [
    "from agentes import *\n",
    "lab = Laberinto(pos_inicial=(11,11))\n",
    "lab.pintar()\n",
    "agente = Agente()\n",
    "agente.perceptos = lab.para_sentidos()\n",
    "print('La lista de sensores es:')\n",
    "print('[sensor frontal, sensor izquierdo, sensor derecho, sensor trasero]')\n",
    "print('Los valores en la casilla (11,11) son:')\n",
    "print(agente.perceptos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las acciones posibles del agente son:\n",
    "\n",
    "* adelante: el agente avanza una casilla.\n",
    "* voltearIzquierda: el agente gira 90º en contra de las manecillas del reloj.\n",
    "* voltearDerecha: el agente gira 90º a favor de las manecillas del reloj.\n",
    "\n",
    "Cada acción del agente tiene un efecto en el entorno, implementado mediante el método `transicion()` de la clase `Laberinto`. Veamos un ejemplo en donde el agente parte de la casilla $(11,11)$ y deambula un poco por el laberinto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from agentes import *\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lab = Laberinto(pos_inicial=(11,11))\n",
    "acciones = ['adelante', 'adelante', 'voltearIzquierda', \n",
    "            'adelante', 'adelante', 'voltearIzquierda', 'adelante']\n",
    "\n",
    "lab.pintar()\n",
    "\n",
    "for a in acciones:\n",
    "    lab.transicion(a)\n",
    "    clear_output(wait=True)\n",
    "    lab.pintar()\n",
    "    plt.show()\n",
    "    sleep(.25)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un agente dirigido por tabla <a class=\"anchor\" id=\"agenteTD\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "El tipo más sencillo de un programa de agente es cuando hacemos una asociación directa entre input y output, en este caso, entre percepción y acción. Esta asociación se puede realizar mediante una tabla. \n",
    "\n",
    "Definimos la siguiente tabla (que hasta ahora sólo está definida parcialmente), la cual vincula perceptos con acciones. La tabla implementa la idea de que si el agente percibe que el frente no está bloqueado y el flanco derecho está bloqueado, entonces avanza una casilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (sensor frontal, sensor izquierdo, sensor derecho, sensor trasero)\n",
    "tabla = {\n",
    "    # Si el frente no está bloqueado y el flanco derecho está\n",
    "    # bloqueado, avanzar una casilla\n",
    "    (False, True, True, True):['adelante'],\n",
    "    (False, True, True, False):['adelante'],\n",
    "    (False, False, True, True):['adelante'],\n",
    "    (False, False, True, False):['adelante'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que esta regla tan simple requiere ser expresada mediante cuatro filas de la tabla.\n",
    "\n",
    "\n",
    "Ahora incluimos el programa dirigido por tabla como el método `programa()` de la clase `Agente`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def programaTD(self):\n",
    "    self.acciones += self.tabla[tuple(self.perceptos)]\n",
    "\n",
    "setattr(Agente, 'programa', programaTD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observemos cómo trabaja el agente que implementa este programa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from agentes import *\n",
    "\n",
    "lab = Laberinto(pos_inicial=(11,11))\n",
    "agente = Agente()\n",
    "agente.tabla = tabla\n",
    "\n",
    "for i in range(20):\n",
    "    agente.perceptos = lab.para_sentidos()\n",
    "    a = agente.reaccionar() # <= ver definición en agentes.py\n",
    "    lab.transicion(a)\n",
    "    clear_output(wait=True)\n",
    "    lab.pintar()\n",
    "    plt.show()\n",
    "    sleep(.25)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El error `KeyError: (True, False, True, False)` ocurre porque la tabla no tiene ninguna fila para el percepto `(True, False, True, False)` y entonces no puede determinar ninguna acción a tomar. ¡Observe que el agente no sabe qué acción tomar en ninguna situación cuando haya un muro enfrente! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 1:**\n",
    "\n",
    "El agente llega hasta que se topa con un muro y no sabe qué hacer. Extienda la tabla anterior para incluir las líneas que determinan que \"si el frente y el flanco derecho están bloqueados y el flanco izquierdo no está bloqueado, voltear a la izquierda\".\n",
    "\n",
    "Visualice el funcionamiento del agente para comprobar su respuesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 2:**\n",
    "\n",
    "En la tabla falta incluir instrucciones que digan que \"si el flanco derecho no está bloqueado, voltear a la derecha y avanzar una casilla\". Extienda la tabla del ejercicio 1 para incluir las líneas que implementen esta regla. Visualice el funcionamiento del agente comenzando desde la casilla $(11,11)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 3:**\n",
    "\n",
    "* ¿Qué pasa cuando el agente comienza en la casilla $(6,3)$? Describa el comportamiento del agente y explique si el programa de agente resulta adecuado o no para este caso.\n",
    "\n",
    "* ¿Qué pasa cuando el agente comienza en la casilla $(7,3)$? Describa el comportamiento del agente y explique si el programa de agente resulta adecuado o no para este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un agente de reflejo simple <a class=\"anchor\" id=\"agenteSR\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "El proceso de escribir una tabla es bastante dispendioso, pues hay que considerar una gran cantidad de combinaciones de valores para los sensores. Un tipo de agente de nivel un poco más elevado que resuelve esta situación son los agentes de reflejo simple. El programa de este tipo de agentes está basado en relgas de condición-acción, las cuales relacionan condiciones sobre los sensores y las acciones. Al considerar condiciones en lugar de combinaciones de valores, la escritura es más eficiente.\n",
    "\n",
    "A continuación presentamos una posible implementación de un agente de reflejo simple para el problema del laberinto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perceptos[0]  =>  sensor forntal\n",
    "# perceptos[1]  =>  sensor izquierdo\n",
    "# perceptos[2]  =>  sensor derecho\n",
    "# perceptos[3]  =>  sensor trasero\n",
    "reglas = {\n",
    "    # Si el frente no está bloqueado y el flanco derecho está\n",
    "    # bloqueado, avanzar una casilla\n",
    "    'not self.perceptos[0] and self.perceptos[2]': ['adelante'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def programaSR(self):\n",
    "    reaccion = self.reglas\n",
    "    for antecedente in self.reglas:\n",
    "        if eval(antecedente):\n",
    "            self.acciones += reaccion[antecedente]\n",
    "            break\n",
    "\n",
    "setattr(Agente, 'programa', programaSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from agentes import *\n",
    "\n",
    "lab = Laberinto(pos_inicial=(11,11))\n",
    "agente = Agente()\n",
    "agente.reglas = reglas\n",
    "\n",
    "for i in range(50):\n",
    "    agente.perceptos = lab.para_sentidos()\n",
    "    a = agente.reaccionar()\n",
    "    lab.transicion(a)\n",
    "    clear_output(wait=True)\n",
    "    lab.pintar()\n",
    "    plt.show()\n",
    "    sleep(.25) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El error `IndexError: pop from empty list` ocurre porque la lista de acciones es vacía, toda vez que el programa aún no está equipado para dar una decisión cuando hay un muro enfrente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 4:**\n",
    "\n",
    "Extienda el programa de agente anterior para implementar las reglas condición-acción siguientes:\n",
    "\n",
    "* Si el frente y el flanco derecho están bloqueados pero el flanco izquierdo no está bloqueado, voltear a la izquierda.\n",
    "* Si el flanco derecho no está bloqueado, voltear a la derecha y avanzar una casilla.\n",
    "\n",
    "Visualice el funcionamiento del agente desde la casilla $(11,11)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 5:**\n",
    "\n",
    "¿Cree usted que es posible implementar un agente de respuesta simple para encontrar una salida al comenzar en las casillas $(6,3)$ y $(7,3)$? Justifique su respuesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un agente basado en conocimiento <a class=\"anchor\" id=\"agenteKB\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Vamos a crear un agente basado en conocimiento para resolver el problema del laberinto. Veremos que no es tan sencillo como el de reflejo simple. El programa de agente en este caso no es una conexión directa entre perceptos y acciones, pues en la mitad está un modelo del mundo. El modelo será un conjunto de enunciados lógicos en una base de conocimiento, como vimos en el notebook anterior. \n",
    "\n",
    "Son varias las ventajas de este tipo de agente respecto al de respuesta simple. Primero, las proposiciones que definen el modelo no requieren estar conectadas directamente a los perceptos y a las acciones. Esto permite definir nuevas categorías para armar restricciones de mayor nivel de abstracción. Además, es posible tener una memoria para almacenar información que pueda ser utilizada muchos turnos después. Adicionalmente, la base de conocimiento permite hacer razonamientos para obtener nueva información a partir de la información almacenada.\n",
    "\n",
    "Para definir el programa requerimos:\n",
    "\n",
    "* Interpretar los perceptos como una fórmula lógica.\n",
    "* Una base de conocimiento con las reglas que definen la dinámica del agente en el mundo.\n",
    "* Una manera de hacer consultas en la base de conocimiento para determinar nueva información.\n",
    "* Un entramado de reglas, como en el programa de reflejo simple, que defina qué va a hacer el agente y cómo usará la información de su base de conocimiento para decidir sus acciones.\n",
    "\n",
    "Veamos ahora uno a uno los componentes del programa de agente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretación de los perceptos\n",
    "\n",
    "La clase `Agente` ya trae definido un método `interp_percepto()`, el cual interpreta los perceptos como letras proposicionales, con la siguiente representación:\n",
    "\n",
    "* frn_bloq es verdadero sii el sensor de enfrente del robot detecta un obstáculo.\n",
    "* izq_bloq es verdadero sii el sensor de la izquierda del robot detecta un obstáculo.\n",
    "* der_bloq es verdadero sii el sensor de la derecha del robot detecta un obstáculo.\n",
    "* atr_bloq es verdadero sii el sensor de atrás del robot detecta un obstáculo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentes import *\n",
    "lab = Laberinto(pos_inicial=(11,11))\n",
    "agente = Agente()\n",
    "agente.perceptos = lab.para_sentidos()\n",
    "agente.interp_percepto()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que cada letra trae un número asociado. Este número es el número del turno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente.turno = 3\n",
    "agente.interp_percepto()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asociar un turno es indispensable, toda vez que en lógica proposicional los valores de las letras proposionales no cambian nunca, pero el valor de los sensores sí. Por esta razón, la representación correcta de los perceptos es la siguiente:\n",
    "\n",
    "* frn_bloq_n es verdadero sii el sensor de enfrente del robot detecta un obstáculo en el turno n.\n",
    "* izq_bloq_n es verdadero sii el sensor de la izquierda del robot no detecta un obstáculo en el turno n.\n",
    "* der_bloq_n es verdadero sii el sensor de la derecha del robot detecta un obstáculo en el turno n.\n",
    "* atr_bloq_n es verdadero sii el sensor de atrás del robot detecta un obstáculo en el turno n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La base de conocimiento\n",
    "\n",
    "El modelo define la dinámica del mundo y del agente. En este caso sencillo, dado que el entorno es estático, sólo requerimos la dinámica del agente. Es decir, solo tenemos que representar lo que debe hacer el agente de acuerdo a alguna condición. Estas reglas estarán descritas mediante una fórmula lógica. \n",
    "\n",
    "Observe que tenemos una restricción importante aquí, toda vez que nuestras fórmulas solo aceptan un cuerpo conformado por conjunciones de literales y una cabeza con un solo literal. La fórmula que representa la regla \"Si el frente no está bloqueado y el flanco derecho está bloqueado, avanzar una casilla\" es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conocimiento(self):\n",
    "    turno = agente.turno\n",
    "    formulas = [\n",
    "        # Si el frente no está bloqueado y el flanco derecho está\n",
    "        # bloqueado, avanzar una casilla\n",
    "        f'-frn_bloq_{turno}Yder_bloq_{turno}>adelante_{turno}',\n",
    "    ]\n",
    "    return formulas\n",
    "\n",
    "setattr(Agente, 'conocimiento', conocimiento)\n",
    "\n",
    "agente = Agente()\n",
    "agente.conocimiento()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** Observe que las letras proposicionales que representan acciones también están indexadas por el turno.\n",
    "\n",
    "Implementamos ahora la fórmula para las reglas\n",
    "\n",
    "* \"Si el frente no está bloqueado y el flanco derecho está bloqueado, avanzar una casilla\"\n",
    "* \"Si el frente y el flanco derecho están bloqueados y el flanco izquierdo no está bloqueado, voltear a la izquierda\"\n",
    "* \"Si el flanco derecho no está bloqueado, voltear a la derecha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conocimiento(self):\n",
    "    turno = agente.turno\n",
    "    formulas = [\n",
    "        # Si el frente no está bloqueado y el flanco derecho está\n",
    "        # bloqueado, avanzar una casilla\n",
    "        f'-frn_bloq_{turno}Yder_bloq_{turno}>adelante_{turno}',\n",
    "        # Si el frente y el flanco derecho están bloqueados y \n",
    "        # el flanco izquierdo no está bloqueado, voltear a la izquierda\n",
    "        f'frn_bloq_{turno}Yder_bloq_{turno}Y-izq_bloq_{turno}>voltearIzquierda_{turno}',\n",
    "        # Si el flanco derecho no está bloqueado, voltear a la derecha \n",
    "        # y avanzar una casilla\n",
    "        f'-der_bloq_{turno}>voltearDerecha_{turno}',\n",
    "    ]\n",
    "    return formulas\n",
    "\n",
    "setattr(Agente, 'conocimiento', conocimiento)\n",
    "\n",
    "agente = Agente()\n",
    "agente.conocimiento()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consultas\n",
    "\n",
    "La manera de usar la base de conocimiento es mediante consultas. El resultado de una consulta es `True` o `False`. Una consulta se hace con un objetivo, el cual requiere un literal y la condición de si este se deduce o no a partir de la información en la base de conocimiento. El siguiente es el pseudocódigo que define la función `ASK`, mediante la cual haremos consultas a la base:\n",
    "\n",
    "<img src=\"./imagenes/ask.png\" width=\"400\"/>\n",
    "\n",
    "Por ejemplo, un objetivo puede ser saber si al deducir `frn_libre_1` a partir de la base de conocimiento, mediante el algoritmo `backward_chaining`, da como resultado `success`. Si sí, la consulta retorna `True`. \n",
    "\n",
    "El otro tipo de objetivo sería saber si el resultado es `failure`. Si `frn_libre_1` no se puede deducir y da como resultado `failure`, entonces el resultado de la consulta es `True`.\n",
    "\n",
    "Observe que la función `ASK` ya está implementada en la librería `Logica`.\n",
    "\n",
    "Veámos las consultas que haremos para nuestro caso particular. En primer lugar, definimos la base de conocimiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logica import *\n",
    "from agentes import *\n",
    "lab = Laberinto()\n",
    "agente = Agente()\n",
    "formulas = agente.conocimiento()\n",
    "base_con = LPQuery(formulas)\n",
    "agente.base = base_con\n",
    "agente.perceptos = lab.para_sentidos()\n",
    "agente.base.TELL(agente.interp_percepto())\n",
    "print(agente.base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora preguntamos si la acción que debemos hacer es voltear a la derecha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objetivo = 'voltearDerecha_'+str(agente.turno)\n",
    "ASK(objetivo, 'success', agente.base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, preguntaremos si debemos seguir adelante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objetivo = 'adelante_'+str(agente.turno)\n",
    "ASK(objetivo, 'success', agente.base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, si la anterior consulta no hubiera funcionado, seguiríamos con la consulta de si debemos voltear a la izquierda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objetivo = 'voltearIzquierda_'+str(agente.turno)\n",
    "ASK(objetivo, 'success', agente.base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición del programa de agente\n",
    "\n",
    "Ya podemos poner todos los elementos juntos en un programa de agente y ver su funcionamiento en el laberinto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def programaKB(self):\n",
    "    turno = self.turno\n",
    "    if ASK(f'voltearDerecha_{turno}', 'success', self.base):\n",
    "        self.acciones.append('voltearDerecha')\n",
    "        self.base.TELL(f'voltearDerecha_{turno}')\n",
    "    elif ASK(f'adelante_{turno}', 'success', self.base):\n",
    "        self.acciones.append('adelante')\n",
    "        self.base.TELL(f'adelante_{turno}')\n",
    "    elif ASK(f'voltearIzquierda_{turno}', 'success', self.base):\n",
    "        self.acciones.append('voltearIzquierda')\n",
    "        self.base.TELL(f'voltearIzquierda_{turno}')\n",
    "    else:\n",
    "        raise Exception('¡Caso no considerado!')\n",
    "setattr(Agente, 'programa', programaKB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estudiemos las primeras tres rondas y lo que pasa con la base de conocimiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from agentes import *\n",
    "\n",
    "lab = Laberinto(pos_inicial=(5,9), dir_agente='este')\n",
    "agente = Agente()\n",
    "formulas = agente.conocimiento()\n",
    "base_con = LPQuery(formulas)\n",
    "agente.base = base_con\n",
    "\n",
    "for i in range(3):\n",
    "    lab.pintar()\n",
    "    agente.perceptos = lab.para_sentidos()\n",
    "    agente.base.TELL(agente.interp_percepto())\n",
    "    for f in agente.conocimiento():\n",
    "        agente.base.TELL(f)\n",
    "    print(f'------- Turno {i+1} -------')\n",
    "    print(agente.base)\n",
    "    print('voltearDerecha?', ASK('voltearDerecha_'+str(agente.turno), 'success', agente.base))\n",
    "    print('adelante?', ASK('adelante_'+str(agente.turno), 'success', agente.base))\n",
    "    print('voltearIzquierda?', ASK('voltearIzquierda_'+str(agente.turno), 'success', agente.base))\n",
    "    a = agente.reaccionar()\n",
    "    lab.transicion(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para finalizar, pongamos a andar al agente por el laberinto desde la casilla $(11,11)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from agentes import *\n",
    "\n",
    "lab = Laberinto(pos_inicial=(11,11))\n",
    "agente = Agente()\n",
    "formulas = agente.conocimiento()\n",
    "base_con = LPQuery(formulas)\n",
    "agente.base = base_con\n",
    "\n",
    "for i in range(30):\n",
    "    agente.perceptos = lab.para_sentidos()\n",
    "    agente.base.TELL(agente.interp_percepto())\n",
    "    for f in agente.conocimiento():\n",
    "        agente.base.TELL(f)\n",
    "    a = agente.reaccionar()\n",
    "    lab.transicion(a)\n",
    "    clear_output(wait=True)\n",
    "    lab.pintar()\n",
    "    plt.show()\n",
    "    sleep(.25) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 6:**\n",
    "\n",
    "Para arreglar el funcionamiento del agente necesitamos hacer lo siguiente:\n",
    "\n",
    "* Modifique la primera regla en la base de conocimiento para que diga \"Si el frente no está bloqueado, avanzar una casilla\".\n",
    "* Modifique la segunda regla en la base de conocimiento para que diga \"Si el frente está bloqueado y la izquierda no, avanzar una casilla\".\n",
    "* Modifique la tercera regla en la base de conocimiento para que diga que \"Si el flanco derecho no está bloqueado pero estaba bloqueado el turno pasado, voltear a la derecha\".\n",
    "\n",
    "\n",
    "Corra de nuevo el programa de agente desde la casilla $(11,11)$ para verificar el funcionamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conocimiento(self):\n",
    "    turno = agente.turno\n",
    "    formulas = [\n",
    "        # Si el frente no está bloqueado, avanzar una casilla\n",
    "\n",
    "        # AQUÍ LA FÓRMULA\n",
    "        \n",
    "        # Si el frente y la izquierda no están bloqueados, \n",
    "        # avanzar una casilla\n",
    "\n",
    "        # AQUÍ LA FÓRMULA\n",
    "        \n",
    "        # Si el flanco derecho no está bloqueado pero estaba\n",
    "        # bloqueado el turno pasado, voltear a la derecha \n",
    "\n",
    "        # AQUÍ LA FÓRMULA\n",
    "\n",
    "    ]\n",
    "    return formulas\n",
    "\n",
    "setattr(Agente, 'conocimiento', conocimiento)\n",
    "\n",
    "agente = Agente()\n",
    "agente.conocimiento()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 7:**\n",
    "\n",
    "¿Desde cuáles de las siguientes casillas el programa que hemos escrito soluciona el laberinto?\n",
    "\n",
    "* $(6,3)$\n",
    "* $(11,2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando un mapa mental<a class=\"anchor\" id=\"mapa\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "El programa de agente basado en conocimiento que hemos escrito permite solucionar el problema desde varias casillas, pero no desde $(3,3)$. Para solucionar el problema del laberinto debemos considerar qué casillas visitamos y cuáles no. Esto es, necesitamos ir creando una especie de mapa mental. Este mapa irá quedando grabado en una serie de letras proposicionales que representan la casilla en que está el agente. Para lograr esto, desarrollaremos la base de conocimiento de la siguiente manera. Primero necesitamos una letra proposicional que represente la posición actual del agente:\n",
    "\n",
    "* en(x,y)_t es verdadero sii el agente está en la casilla $(x,y)$ en el turno $t$.\n",
    "\n",
    "Para controlar la posición del agente, es necesario usar la información de qué acción realiza y en qué dirección está mirando. Entonces necesitamos cuatro letras proposicionales que representen las cuatro direcciones cardinales:\n",
    "\n",
    "* midando_o_t es verdadero sii el agente está mirando al oeste en el turno $t$.\n",
    "* midando_e_t es verdadero sii el agente está mirando al este en el turno $t$.\n",
    "* midando_s_t es verdadero sii el agente está mirando al sur en el turno $t$.\n",
    "* midando_n_t es verdadero sii el agente está mirando al norte en el turno $t$.\n",
    "\n",
    "### Posición del agente (fluente_en)\n",
    "La posición del agente de un turno a otro está dada por las siguientes regas:\n",
    "\n",
    "* en($x$,$y$)$_t$ $\\wedge$ mirando_o$_t$ $\\wedge$ adelante$_t$ $\\to$ en($x-1$,$y$)$_{t+1}$\n",
    "* en($x$,$y$)$_t$ $\\wedge$ mirando_e$_t$ $\\wedge$ adelante$_t$ $\\to$ en($x+1$,$y$)$_{t+1}$\n",
    "* en($x$,$y$)$_t$ $\\wedge$ mirando_s$_t$ $\\wedge$ adelante$_t$ $\\to$ en($x$,$y-1$)$_{t+1}$\n",
    "* en($x$,$y$)$_t$ $\\wedge$ mirando_n$_t$ $\\wedge$ adelante$_t$ $\\to$ en($x$,$y+1$)$_{t+1}$\n",
    "* en($x$,$y$)$_t$ $\\wedge$ voltearIzquierda $\\to$ en($x$,$y$)$_t$\n",
    "* en($x$,$y$)$_t$ $\\wedge$ voltearDerecha $\\to$ en($x$,$y$)$_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fluente_en(self):\n",
    "    turno = agente.turno\n",
    "    x, y = agente.loc\n",
    "    formulas = [\n",
    "        f'en({x},{y})_{turno}Ymirando_o_{turno}Yadelante_{turno}>en({x-1},{y})_{turno+1}',\n",
    "        f'en({x},{y})_{turno}Ymirando_e_{turno}Yadelante_{turno}>en({x+1},{y})_{turno+1}',\n",
    "        f'en({x},{y})_{turno}Ymirando_s_{turno}Yadelante_{turno}>en({x},{y-1})_{turno+1}',\n",
    "        f'en({x},{y})_{turno}Ymirando_n_{turno}Yadelante_{turno}>en({x},{y+1})_{turno+1}',\n",
    "        f'en({x},{y})_{turno}YvoltearIzquierda_{turno}>en({x},{y})_{turno+1}',\n",
    "        f'en({x},{y})_{turno}YvoltearDerecha_{turno}>en({x},{y})_{turno+1}',\n",
    "    ]\n",
    "    return formulas\n",
    "\n",
    "setattr(Agente, 'fluente_en', fluente_en)\n",
    "\n",
    "agente = Agente()\n",
    "agente.fluente_en()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dirección en que mira (fluente_mirando)\n",
    "\n",
    "Veamos ahora las reglas que determinan el comportamiento de la dirección del agente. Las primeras dicen que la acción de avanzar una casilla no cambia la dirección:\n",
    "\n",
    "* mirando_o$_t$ $\\wedge$ adelante$_t$ $\\to$ mirando_o$_{t+1}$\n",
    "* mirando_s$_t$ $\\wedge$ adelante$_t$ $\\to$ mirando_s$_{t+1}$\n",
    "* mirando_e$_t$ $\\wedge$ adelante$_t$ $\\to$ mirando_e$_{t+1}$\n",
    "* mirando_n$_t$ $\\wedge$ adelante$_t$ $\\to$ mirando_n$_{t+1}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fluente_mirando(turno):\n",
    "    turno = agente.turno\n",
    "    formulas = [\n",
    "        f'mirando_o_{turno}Yadelante_{turno}>mirando_o_{turno+1}',\n",
    "        f'mirando_s_{turno}Yadelante_{turno}>mirando_s_{turno+1}',\n",
    "        f'mirando_e_{turno}Yadelante_{turno}>mirando_e_{turno+1}',\n",
    "        f'mirando_n_{turno}Yadelante_{turno}>mirando_n_{turno+1}',\n",
    "    ]\n",
    "    return formulas\n",
    "\n",
    "setattr(Agente, 'fluente_mirando', fluente_mirando)\n",
    "\n",
    "agente = Agente()\n",
    "agente.fluente_mirando()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También tenemos las siguientes reglas, que controlan el cambio de dirección con la acción de voltear 90º a la izquierda:\n",
    "\n",
    "* mirando_o$_t$ $\\wedge$ voltearIzquierda$_t$ $\\to$ mirando_s$_{t+1}$\n",
    "* mirando_s$_t$ $\\wedge$ voltearIzquierda$_t$ $\\to$ mirando_e$_{t+1}$\n",
    "* mirando_e$_t$ $\\wedge$ voltearIzquierda$_t$ $\\to$ mirando_n$_{t+1}$\n",
    "* mirando_n$_t$ $\\wedge$ voltearIzquierda$_t$ $\\to$ mirando_o$_{t+1}$\n",
    "\n",
    "Y lo mismo con la acción de voltear a la derecha:\n",
    "\n",
    "* mirando_o$_t$ $\\wedge$ voltearDerecha$_t$ $\\to$ mirando_n$_{t+1}$\n",
    "* mirando_n$_t$ $\\wedge$ voltearDerecha$_t$ $\\to$ mirando_e$_{t+1}$\n",
    "* mirando_e$_t$ $\\wedge$ voltearDerecha$_t$ $\\to$ mirando_s$_{t+1}$\n",
    "* mirando_s$_t$ $\\wedge$ voltearDerecha$_t$ $\\to$ mirando_o$_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 8:**\n",
    "\n",
    "Implemente los dos últimos tipos de reglas para el fuente $mirando$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casillas visitadas (fluente_visitadas)\n",
    "\n",
    "Ahora ya podemos incluir letras proposicionales que determinen que una casilla ha sido visitada:\n",
    "\n",
    "* visitada(x,y)_t es verdadera sii el agente ha visitado la casilla $(x,y)$ en el turno $t$.\n",
    "\n",
    "Las reglas son las siguientes:\n",
    "\n",
    "* en(x,y)$_t$ $\\to$ visitada(x,y)$_t$\n",
    "* visitada(x,y)$_t$ $\\to$ visitada(x,y)$_{t+1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fluente_visitadas(self):\n",
    "    turno = self.turno\n",
    "    casillas = [(x,y) for x in range(12) for y in range(12)]\n",
    "    formulas = []\n",
    "    for c in casillas:\n",
    "        x, y = c\n",
    "        formulas += [\n",
    "            f'en({x},{y})_{turno}>visitada({x},{y})_{turno}',                \n",
    "            f'visitada({x},{y})_{turno}>visitada({x},{y})_{turno+1}',                \n",
    "        ]\n",
    "    return formulas\n",
    "\n",
    "setattr(Agente, 'fluente_visitadas', fluente_visitadas)\n",
    "\n",
    "agente = Agente()\n",
    "agente.fluente_visitadas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incompatibilidad de reglas\n",
    "\n",
    "Observe que las reglas 1 y 3 de la base de conocimiento \n",
    "\n",
    "$\\neg$frn_bloq$_t$ $\\to$ adelante$_t$\n",
    "\n",
    "y \n",
    "\n",
    "$\\neg$der_bloq$_t$ $\\wedge$ der_bloq$_{t-1}$ $\\to$ voltearDerecha$_t$\n",
    "\n",
    "producen acciones contradictorias en una situación en la cual la fórmula\n",
    "\n",
    "$\\neg$frn_bloq$_t$ $\\wedge$ $\\neg$der_bloq$_t$ $\\wedge$ der_bloq$_{t-1}$\n",
    "\n",
    "sea verdadera.\n",
    "\n",
    "El programa de agente `programaKB` está diseñado para primero verificar si el agente voltea a la derecha. En caso afirmativo, no verifica más opciones. No obstante, **la base de conocimiento permite derivar las dos acciones**.\n",
    "\n",
    "Es necesario arreglar esta situación, de tal manera que una y solo una regla sea aplicable en cada situación. Esto se logra modificando la primera regla, de tal manera que se cumpla en los casos en que no se cumple la tercera. Esto se logra mediante la fórmula:\n",
    "\n",
    "$\\neg$frn_bloq$_t$ $\\wedge$ (der_bloq$_t$ $\\vee$ $\\neg$der_bloq$_{t-1}$) $\\to$ adelante$_t$\n",
    "\n",
    "Como esta regla transgrede la restricción que hemos hecho sobre las reglas, debemos modificarla. El mismo efecto se logra usando dos reglas así:\n",
    "\n",
    "* $\\neg$frn_bloq$_t$ $\\wedge$ der_bloq$_t$ $\\to$ adelante$_t$\n",
    "* $\\neg$frn_bloq$_t$ $\\wedge$ $\\neg$der_bloq$_t$ $\\wedge$ $\\neg$der_bloq$_{t-1}$ $\\to$ adelante$_t$\n",
    "\n",
    "Es necesario hacer lo mismo con la segunda regla. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conocimiento(self):\n",
    "    turno = agente.turno\n",
    "    formulas = [\n",
    "        # Si el frente no está bloqueado Y\n",
    "        #    -  la derecha está bloqueada O\n",
    "        #    -  la derecha no esta bloqueada ahora ni en el turno pasado,\n",
    "        # entonces avanzar una casilla\n",
    "        f'-frn_bloq_{turno}Yder_bloq_{turno}>adelante_{turno}',\n",
    "        f'-frn_bloq_{turno}Y-der_bloq_{turno}Y-der_bloq_{turno-1}>adelante_{turno}',\n",
    "        # Si el frente está bloqueado y la izquierda no Y\n",
    "        #    -  la derecha está bloqueada O\n",
    "        #    -  la derecha no esta bloqueada ahora ni en el turno pasado, \n",
    "        # entonces voltear a la izquierda\n",
    "        f'frn_bloq_{turno}Y-izq_bloq_{turno}Yder_bloq_{turno}>voltearIzquierda_{turno}',\n",
    "        f'frn_bloq_{turno}Y-izq_bloq_{turno}Y-der_bloq_{turno}Y-der_bloq_{turno-1}>voltearIzquierda_{turno}',\n",
    "        # Si el flanco derecho no está bloqueado pero estaba\n",
    "        # bloqueado el turno pasado, voltear a la derecha \n",
    "        f'-der_bloq_{turno}Yder_bloq_{turno-1}>voltearDerecha_{turno}',\n",
    "    ]\n",
    "    return formulas\n",
    "\n",
    "setattr(Agente, 'conocimiento', conocimiento)\n",
    "\n",
    "agente = Agente()\n",
    "agente.conocimiento()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimación de estados\n",
    "\n",
    "La base de conocimiento crece cada turno, puesto que vamos incluyendo cada vez más y más reglas y datos en ella. Esto genera, evidentemente, que las consultas sean cada vez más demoradas. Para solucionar este inconveniente, debemos mantener un tamaño relativamente constante de la base de conocimiento. Esto lo logramos mediante una estimación de estado, que consiste en solo guardar la información relevante en memoria del estado actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posicion_inicial(self):\n",
    "    x, y = self.loc\n",
    "    formulas = ['mirando_o_1', \n",
    "                f'en({x},{y})_1',\n",
    "                '-frn_visitada_1',\n",
    "                '-frn_bloq_0',\n",
    "                '-izq_bloq_0',\n",
    "                '-der_bloq_0',\n",
    "                '-atr_bloq_0',\n",
    "               ]\n",
    "    return formulas\n",
    "\n",
    "setattr(Agente,'posicion_inicial',posicion_inicial)\n",
    "\n",
    "def estimar_estado(self):\n",
    "    turno = self.turno\n",
    "    formulas = []\n",
    "    formulas += self.conocimiento()\n",
    "    formulas += self.fluente_en()\n",
    "    formulas += self.fluente_mirando()\n",
    "    formulas += self.fluente_visitadas()\n",
    "    formulas += self.fluente_frn_visitada()\n",
    "    formulas += [self.nueva_posicion()]\n",
    "    formulas += [self.nueva_direccion()]\n",
    "    formulas += self.cache()\n",
    "    agente.perceptos = lab.para_sentidos()\n",
    "    formulas += [agente.interp_percepto()]\n",
    "    self.base = LPQuery(formulas)\n",
    "  \n",
    "setattr(Agente,'estimar_estado',estimar_estado)\n",
    "\n",
    "def cache(self):\n",
    "    turno = self.turno\n",
    "    # Guardamos los perceptos del turno pasado\n",
    "    aux = [x for x in self.base.datos if f'_bloq_{turno-1}' in x]\n",
    "    # Guardamos las casillas visitadas\n",
    "    visitadas = []\n",
    "    casillas = [(x,y) for x in range(12) for y in range(12)]\n",
    "    for c in casillas:\n",
    "        x, y = c\n",
    "        consulta = ASK(f'visitada({x},{y})_{turno}', 'success', self.base)\n",
    "        if consulta:\n",
    "            visitadas.append(f'visitada({x},{y})_{turno}')\n",
    "    return aux + visitadas\n",
    "\n",
    "setattr(Agente,'cache',cache)\n",
    "\n",
    "def nueva_posicion(self):\n",
    "    casillas = [self.loc] + adyacentes(self.loc)\n",
    "    for c in casillas:\n",
    "        x, y = c\n",
    "        pos = f'en({x},{y})_{self.turno}'\n",
    "        evaluacion = ASK(pos, 'success', self.base)\n",
    "        if evaluacion:\n",
    "            self.loc = (x,y)\n",
    "            return pos\n",
    "    raise Exception('¡No se encontró posición!')\n",
    "\n",
    "setattr(Agente,'nueva_posicion',nueva_posicion)\n",
    "\n",
    "def nueva_direccion(self):\n",
    "    direcciones = ['o', 'e', 's', 'n']\n",
    "    for d in direcciones:\n",
    "        direccion = f'mirando_{d}_{self.turno}'\n",
    "        evaluacion = ASK(direccion, 'success', self.base)\n",
    "        if evaluacion:\n",
    "            return direccion\n",
    "    raise Exception('¡No se encontró dirección!')\n",
    "            \n",
    "setattr(Agente,'nueva_direccion',nueva_direccion)\n",
    "\n",
    "def adyacentes(c):\n",
    "    x, y = c\n",
    "    return [(x-1,y), (x+1,y), (x,y-1), (x,y+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from agentes import *\n",
    "\n",
    "inicial = (6,3)\n",
    "lab = Laberinto(pos_inicial=inicial)\n",
    "agente = Agente()\n",
    "agente.loc = inicial\n",
    "agente.base = LPQuery([])\n",
    "agente.perceptos = lab.para_sentidos()\n",
    "agente.base.TELL(agente.interp_percepto())\n",
    "formulas = []\n",
    "formulas += agente.posicion_inicial()\n",
    "formulas += agente.conocimiento()\n",
    "formulas += agente.fluente_en()\n",
    "formulas += agente.fluente_mirando()\n",
    "formulas += agente.fluente_visitadas()\n",
    "for f in formulas:\n",
    "    agente.base.TELL(f)\n",
    "\n",
    "lab.pintar()\n",
    "plt.show()\n",
    "sleep(.5) \n",
    "    \n",
    "for i in range(25):\n",
    "    a = agente.reaccionar()\n",
    "    lab.transicion(a)\n",
    "    agente.nueva_direccion()\n",
    "    agente.nueva_posicion()\n",
    "    agente.estimar_estado()\n",
    "    clear_output(wait=True)\n",
    "    print(vis(agente))\n",
    "    lab.pintar()\n",
    "    plt.show()\n",
    "    sleep(.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorías adicionales (fluente_frn_visitada)\n",
    "\n",
    "Para ilustrar cómo, mediante la lógica, podemos crear categorías cada vez más abstractas, incluiremos una letra proposicional que representa que la casilla de enfrente del agente, en la posición en que se encuentra y en la dirección en que está mirando, ha sido visitada por el agente en algún turno anterior:\n",
    "\n",
    "* frn_visitada$_t$ es verdadera sii en el turno $t$ el agente ya ha visitado la casilla enfrente suyo.\n",
    "\n",
    "Las reglas que controlan esta letra son:\n",
    "\n",
    "* en(x,y)$_t$ $\\wedge$ mirando_o$_t$ $\\wedge$ visitada(x-1,y)$_t$ $\\to$ frn_visitada$_t$\n",
    "* en(x,y)$_t$ $\\wedge$ mirando_e$_t$ $\\wedge$ visitada(x+1,y)$_t$ $\\to$ frn_visitada$_t$\n",
    "* en(x,y)$_t$ $\\wedge$ mirando_s$_t$ $\\wedge$ visitada(x,y-1)$_t$ $\\to$ frn_visitada$_t$\n",
    "* en(x,y)$_t$ $\\wedge$ mirando_n$_t$ $\\wedge$ visitada(x,y+1)$_t$ $\\to$ frn_visitada$_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fluente_frn_visitada(self):\n",
    "    turno = self.turno\n",
    "    x, y = self.loc\n",
    "    formulas = [\n",
    "        f'en({x},{y})_{turno}Ymirando_o_{turno}Yvisitada({x-1},{y})_{turno}>frn_visitada_{turno}',        \n",
    "        f'en({x},{y})_{turno}Ymirando_e_{turno}Yvisitada({x+1},{y})_{turno}>frn_visitada_{turno}',        \n",
    "        f'en({x},{y})_{turno}Ymirando_s_{turno}Yvisitada({x},{y-1})_{turno}>frn_visitada_{turno}',        \n",
    "        f'en({x},{y})_{turno}Ymirando_n_{turno}Yvisitada({x},{y+1})_{turno}>frn_visitada_{turno}',        \n",
    "    ]\n",
    "    return formulas\n",
    "\n",
    "setattr(Agente, 'fluente_frn_visitada', fluente_frn_visitada)\n",
    "\n",
    "agente = Agente()\n",
    "agente.fluente_frn_visitada()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def estimar_estado(self):\n",
    "    turno = self.turno\n",
    "    formulas = []\n",
    "    formulas += self.conocimiento()\n",
    "    formulas += self.fluente_en()\n",
    "    formulas += self.fluente_mirando()\n",
    "    formulas += self.fluente_visitadas()\n",
    "    formulas += self.fluente_frn_visitada() # <= Incluimos este fluente en la estimación de estado \n",
    "    formulas += [self.nueva_posicion()]\n",
    "    formulas += [self.nueva_direccion()]\n",
    "    formulas += self.cache()\n",
    "    agente.perceptos = lab.para_sentidos()\n",
    "    formulas += [agente.interp_percepto()]\n",
    "    self.base = LPQuery(formulas)\n",
    "    # Deducimos si la casilla de enfrente ha sido visitada\n",
    "    if ASK(f'frn_visitada_{turno}', 'success', self.base):\n",
    "        self.base.TELL(f'frn_visitada_{turno}')\n",
    "    else:\n",
    "        self.base.TELL(f'-frn_visitada_{turno}') # <= Ver nota importante abajo\n",
    "  \n",
    "setattr(Agente,'estimar_estado',estimar_estado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota importante:** Observe que los métodos de deducción que hemos desarrollado son incompletos. Esto quiere decir, en nuestro caso, que es posible que de la base de datos no se deduzca la proposición frn_visitada$_t$ ni su negación $\\neg$frn_visitada$_t$. Es por eso que si la consulta con el objetivo frn_visitada$_t$ y `success` es `False`, incluimos a mano la negación, es decir, $\\neg$frn_visitada$_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from agentes import *\n",
    "\n",
    "inicial = (6,3)\n",
    "lab = Laberinto(pos_inicial=inicial)\n",
    "agente = Agente()\n",
    "agente.loc = inicial\n",
    "agente.base = LPQuery([])\n",
    "agente.perceptos = lab.para_sentidos()\n",
    "agente.base.TELL(agente.interp_percepto())\n",
    "formulas = []\n",
    "formulas += agente.posicion_inicial()\n",
    "formulas += agente.conocimiento()\n",
    "formulas += agente.fluente_en()\n",
    "formulas += agente.fluente_mirando()\n",
    "formulas += agente.fluente_visitadas()\n",
    "formulas += agente.fluente_frn_visitada()\n",
    "for f in formulas:\n",
    "    agente.base.TELL(f)\n",
    "\n",
    "lab.pintar()\n",
    "plt.show()\n",
    "sleep(.5) \n",
    "    \n",
    "for i in range(19):\n",
    "    a = agente.reaccionar()\n",
    "    lab.transicion(a)\n",
    "    agente.nueva_direccion()\n",
    "    agente.nueva_posicion()\n",
    "    agente.estimar_estado()\n",
    "    clear_output(wait=True)\n",
    "    print(vis(agente))\n",
    "    lab.pintar()\n",
    "    plt.show()\n",
    "    sleep(.15) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para tratar de resolver el laberinto desde cada vez más posiciones iniciales, incluimos en las reglas que si el frente ya ha sido visitado, voltee a la derecha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def conocimiento(self):\n",
    "    turno = agente.turno\n",
    "    formulas = [\n",
    "        # Si el frente ha sido visitado, voltear a la izquierda\n",
    "        f'frn_visitada_{turno}>voltearIzquierda_{turno}',\n",
    "        # Si el frente no ha sido visitado y no está bloqueado Y\n",
    "        #    -  la derecha está bloqueada O\n",
    "        #    -  la derecha no esta bloqueada ahora ni en el turno pasado\n",
    "        # entonces, avanzar una casilla\n",
    "        f'-frn_visitada_{turno}Y-frn_bloq_{turno}Yder_bloq_{turno}>adelante_{turno}',\n",
    "        f'-frn_visitada_{turno}Y-frn_bloq_{turno}Y-der_bloq_{turno}Y-der_bloq_{turno-1}>adelante_{turno}',\n",
    "        # Si el frente está bloqueado y la izquierda no Y\n",
    "        #    -  la derecha está bloqueada O\n",
    "        #    -  la derecha no esta bloqueada ahora ni en el turno pasado, \n",
    "        # entonces avanzar una casilla\n",
    "        f'frn_bloq_{turno}Y-izq_bloq_{turno}Yder_bloq_{turno}>voltearIzquierda_{turno}',\n",
    "        f'frn_bloq_{turno}Y-izq_bloq_{turno}Y-der_bloq_{turno}Y-der_bloq_{turno-1}>voltearIzquierda_{turno}',\n",
    "        # Si el flanco derecho no está bloqueado pero estaba\n",
    "        # bloqueado el turno pasado, voltear a la derecha \n",
    "        f'-frn_visitada_{turno}Y-der_bloq_{turno}Yder_bloq_{turno-1}>voltearDerecha_{turno}',\n",
    "    ]\n",
    "    return formulas\n",
    "\n",
    "setattr(Agente, 'conocimiento', conocimiento)\n",
    "\n",
    "agente = Agente()\n",
    "agente.conocimiento()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from agentes import *\n",
    "\n",
    "inicial = (6,3)\n",
    "lab = Laberinto(pos_inicial=inicial)\n",
    "agente = Agente()\n",
    "agente.loc = inicial\n",
    "agente.base = LPQuery([])\n",
    "agente.perceptos = lab.para_sentidos()\n",
    "agente.base.TELL(agente.interp_percepto())\n",
    "formulas = []\n",
    "formulas += agente.posicion_inicial()\n",
    "formulas += agente.conocimiento()\n",
    "formulas += agente.fluente_en()\n",
    "formulas += agente.fluente_mirando()\n",
    "formulas += agente.fluente_visitadas()\n",
    "formulas += agente.fluente_frn_visitada()\n",
    "for f in formulas:\n",
    "    agente.base.TELL(f)\n",
    "\n",
    "lab.pintar()\n",
    "plt.show()\n",
    "sleep(.5) \n",
    "    \n",
    "for i in range(90):\n",
    "    a = agente.reaccionar()\n",
    "    lab.transicion(a)\n",
    "    agente.nueva_direccion()\n",
    "    agente.nueva_posicion()\n",
    "    agente.estimar_estado()\n",
    "    clear_output(wait=True)\n",
    "    lab.pintar()\n",
    "    plt.show()\n",
    "    sleep(0.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 9:**\n",
    "\n",
    "Modifique la base de conocimiento, especialmente el método `conocimiento` para que el agente logre salir desde la casilla $(7,3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En este notebook usted aprendió\n",
    "\n",
    "* Los detalles de la implementación de un programa de agente dirigido por tablas, de reflejo simple y basado en el conocimiento.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
